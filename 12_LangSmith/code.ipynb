{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langsmith > /dev/null\n",
    "!pip install langchain > /dev/null\n",
    "!pip install openai > /dev/null\n",
    "!pip install python-dotenv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.predict(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "tracer = LangChainTracer(project_name=\"My Project\")\n",
    "llm.predict(\"How many people live in USA?\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled(project_name=\"My Project\"):\n",
    "    llm.predict(\"How many people live in USA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, tags=[\"my-llm-tag\"])\n",
    "prompt = PromptTemplate.from_template(\"Say {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, tags=[\"one-tag\", \"another-tag\"])\n",
    "\n",
    "chain(\"Hello, World!\", tags=[\"shared-tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    trace_as_chain_group\n",
    ")\n",
    "\n",
    "with trace_as_chain_group(\"my_group_name\") as group_manager:\n",
    "    pass\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"What is the answer to {question}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "with trace_as_chain_group(\"my_group\") as group_manager:\n",
    "    chain.run(question=\"What is your name?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your quest?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your favorite color?\", callbacks=group_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "project_runs = client.list_runs(project_name=\"default\")\n",
    "project_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "todays_runs = client.list_runs(\n",
    "    start_time=datetime.now() - timedelta(days=1),\n",
    "    run_type=\"llm\",\n",
    ")\n",
    "todays_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in todays_runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "Add metadata to filter runs later, for example for making an A/B test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "chain = LLMChain.from_string(llm=chat_model, template=\"What's the answer to {input}?\")\n",
    "\n",
    "chain({\"input\": \"What is the meaning of life?\"}, metadata={\"source\": \"korean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "runs = list(client.list_runs(\n",
    "    filter='has(metadata, \\'{\"source\": \"korean\"}\\')',\n",
    "))\n",
    "print(list(runs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
