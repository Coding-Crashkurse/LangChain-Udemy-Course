{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langsmith > /dev/null\n",
    "!pip install langchain > /dev/null\n",
    "!pip install openai > /dev/null\n",
    "!pip install python-dotenv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "tracer = LangChainTracer(project_name=\"My Project\")\n",
    "llm.predict(\"How many people live in USA?\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled(project_name=\"My Project\"):\n",
    "    llm.predict(\"How many people live in Germany?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, tags=[\"my-llm-tag\"])\n",
    "prompt = PromptTemplate.from_template(\"Say {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, tags=[\"one-tag\", \"another-tag\"])\n",
    "\n",
    "chain(\"Hello, World!\", tags=[\"shared-tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    trace_as_chain_group\n",
    ")\n",
    "\n",
    "with trace_as_chain_group(\"my_group_name\") as group_manager:\n",
    "    pass\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"What is the answer to {question}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "with trace_as_chain_group(\"my_group\") as group_manager:\n",
    "    chain.run(question=\"What is your name?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your quest?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your favorite color?\", callbacks=group_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "project_runs = client.list_runs(project_name=\"default\")\n",
    "project_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "todays_runs = client.list_runs(\n",
    "    project_name=\"default\",\n",
    "    start_time=datetime.now() - timedelta(days=1),\n",
    "    run_type=\"llm\",\n",
    ")\n",
    "todays_runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in todays_runs:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "Add metadata to filter runs later, for example for making an A/B test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "chain = LLMChain.from_string(llm=chat_model, template=\"What's the answer to {input}?\")\n",
    "\n",
    "chain({\"input\": \"What is the meaning of life?\"}, metadata={\"source\": \"korean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "runs = list(client.list_runs(\n",
    "    project_name=\"default\",\n",
    "    filter='has(metadata, \\'{\"source\": \"kkrean\"}\\')',\n",
    "))\n",
    "print(list(runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "dataset = client.create_dataset(dataset_name=\"testdataset\", description=\"A dataset with key-value inputs and outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_example(\n",
    "  inputs={\n",
    "    \"a-question\": \"What is the largest mammal?\",\n",
    "    \"user-context\": \"The user is a 1st grader writing a bio report.\",\n",
    "  },\n",
    "  outputs = {\n",
    "    \"answer\": \"The blue whale is the largest mammal.\",\n",
    "    \"source\": \"https://en.wikipedia.org/wiki/Blue_whale\",\n",
    "  },\n",
    "  dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = client.upload_csv(\n",
    "    csv_file=\"./extended_questions_answers.csv\",\n",
    "    input_keys=[\"Question\"],\n",
    "    output_keys=[\"Answer\"],\n",
    "    name=\"My CSV Dataset\",\n",
    "    description=\"Dataset created from a CSV file\",\n",
    "    data_type=\"kv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",\n",
    "        \"context_qa\",\n",
    "        \"cot_qa\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "llm = ChatOpenAI()\n",
    "run_on_dataset(\n",
    "    dataset_name=\"My CSV Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    client=client,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=\"evalproject\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
